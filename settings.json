{"algorithm": "monte_carlo", "episodes": 200, "epsilon": 0.1, "gamma": 0.99, "max_steps": 100, "alpha": 0.1, "actor_lr": 0.001, "critic_lr": 0.001, "beta": 2.3, "entropy_weight": 0.01, "value_weight": 0.5, "advantage_weight": 1.0, "step_penalty": -0.1, "trash_reward": 10.0, "mine_penalty": -10.0, "evil_penalty": -10.0, "exit_reward": 50.0, "show_reward_graph": true, "show_heatmap": true, "show_value_map": true}